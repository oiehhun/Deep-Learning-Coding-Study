{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyN7hyjjVvkI1t6l2VkvV6AN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Module"],"metadata":{"id":"OHTeQfQZnUqp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3xsZAsbnHDL"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import re\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"4bpa6jOlZSEZ"}},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"B0UJiJzFZJs7","executionInfo":{"status":"ok","timestamp":1689861535703,"user_tz":-540,"elapsed":901,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"f576b2af-e7ef-476a-c5d8-c66db8d3e3db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 Q            A  label\n","0           12시 땡!   하루가 또 가네요.      0\n","1      1지망 학교 떨어졌어    위로해 드립니다.      0\n","2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n","3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n","4          PPL 심하네   눈살이 찌푸려지죠.      0"],"text/html":["\n","\n","  <div id=\"df-9130d682-81bf-4dff-ae26-2eed7a15e36f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q</th>\n","      <th>A</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12시 땡!</td>\n","      <td>하루가 또 가네요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1지망 학교 떨어졌어</td>\n","      <td>위로해 드립니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3박4일 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3박4일 정도 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PPL 심하네</td>\n","      <td>눈살이 찌푸려지죠.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9130d682-81bf-4dff-ae26-2eed7a15e36f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-6c056b4b-b7a6-4dc6-8707-a07740df68bf\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c056b4b-b7a6-4dc6-8707-a07740df68bf')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-6c056b4b-b7a6-4dc6-8707-a07740df68bf button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9130d682-81bf-4dff-ae26-2eed7a15e36f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9130d682-81bf-4dff-ae26-2eed7a15e36f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJRim1ydZ-If","executionInfo":{"status":"ok","timestamp":1689861535703,"user_tz":-540,"elapsed":7,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"cad0b421-665b-4fba-aac0-5e4684992f99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11823, 3)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXEDf1HeZ-7g","executionInfo":{"status":"ok","timestamp":1689861535704,"user_tz":-540,"elapsed":6,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"172d6430-1275-435e-a8b1-1eb424ed07b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Q        0\n","A        0\n","label    0\n","dtype: int64"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"T5qybW2VZrFY"}},{"cell_type":"code","source":["texts = []\n","pairs = []\n","\n","for i in range(len(df)):\n","  texts.append(df.iloc[i,0])\n","  pairs.append(df.iloc[i,1])"],"metadata":{"id":"_lB-Zs2qZTmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 특수 문자 제거\n","def clean_sentence(sentence):\n","    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n","    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n","    return sentence"],"metadata":{"id":"eTBu_vp4Z4wb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y83-mAKFaEvE","executionInfo":{"status":"ok","timestamp":1689862166784,"user_tz":-540,"elapsed":4094,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"f0d01848-4f2b-4d3e-ad37-6638a6eee7b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","okt = Okt()"],"metadata":{"id":"e7edknXjaKmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 형태소 반환\n","def process_morph(sentence):\n","    return ' '.join(okt.morphs(sentence))"],"metadata":{"id":"a5oh2tQkaPox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_and_morph(sentence, is_question=True):\n","    # 한글 문장 전처리\n","    sentence = clean_sentence(sentence)\n","    # 형태소 변환\n","    sentence = process_morph(sentence)\n","    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n","    if is_question:\n","        return sentence\n","    else:\n","        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n","        return ('<START> ' + sentence, sentence + ' <END>')"],"metadata":{"id":"rrcjbpWFaSzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(texts, pairs):\n","    questions = []\n","    answer_in = []\n","    answer_out = []\n","\n","    # 질의에 대한 전처리\n","    for text in texts:\n","        # 전처리와 morph 수행\n","        question = clean_and_morph(text, is_question=True)\n","        questions.append(question)\n","\n","    # 답변에 대한 전처리\n","    for pair in pairs:\n","        # 전처리와 morph 수행\n","        in_, out_ = clean_and_morph(pair, is_question=False)\n","        answer_in.append(in_)\n","        answer_out.append(out_)\n","\n","    return questions, answer_in, answer_out"],"metadata":{"id":"X_fRDhxmaVFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["questions, answer_in, answer_out = preprocess(texts, pairs)"],"metadata":{"id":"heCCw66FaWgm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(questions[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-wwRAUIaXtK","executionInfo":{"status":"ok","timestamp":1689862196151,"user_tz":-540,"elapsed":23,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"9736f9cb-18e0-4800-9622-e52689fba67f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', '심하네']\n"]}]},{"cell_type":"code","source":["print(answer_in[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XITW_z-Xae85","executionInfo":{"status":"ok","timestamp":1689862196151,"user_tz":-540,"elapsed":13,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"2f1f39cb-3010-42f5-d58f-8192fb296e57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<START> 하루 가 또 가네요', '<START> 위로 해 드립니다', '<START> 여행 은 언제나 좋죠', '<START> 여행 은 언제나 좋죠', '<START> 눈살 이 찌푸려지죠']\n"]}]},{"cell_type":"code","source":["print(answer_out[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TwYr3m_agH8","executionInfo":{"status":"ok","timestamp":1689862196151,"user_tz":-540,"elapsed":10,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"8ace5de7-72d6-4295-f8cd-5e28eef248de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['하루 가 또 가네요 <END>', '위로 해 드립니다 <END>', '여행 은 언제나 좋죠 <END>', '여행 은 언제나 좋죠 <END>', '눈살 이 찌푸려지죠 <END>']\n"]}]},{"cell_type":"code","source":["all_sentences = questions + answer_in + answer_out"],"metadata":{"id":"oZqgN18CamfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizer\n","tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')"],"metadata":{"id":"Zjylg7vea8n1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.fit_on_texts(all_sentences)"],"metadata":{"id":"5PKdSe4UbI4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word, idx in tokenizer.word_index.items():\n","    print(f'{word}\\t\\t => \\t{idx}')\n","    if idx > 10:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MNinHlEbKDa","executionInfo":{"status":"ok","timestamp":1689862196152,"user_tz":-540,"elapsed":9,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"ea22ffea-5d74-4eab-c510-141e82797474"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<OOV>\t\t => \t1\n","<START>\t\t => \t2\n","<END>\t\t => \t3\n","이\t\t => \t4\n","을\t\t => \t5\n","거\t\t => \t6\n","가\t\t => \t7\n","예요\t\t => \t8\n","사람\t\t => \t9\n","요\t\t => \t10\n","에\t\t => \t11\n"]}]},{"cell_type":"code","source":["len(tokenizer.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWfTVPRwbOqv","executionInfo":{"status":"ok","timestamp":1689862196152,"user_tz":-540,"elapsed":7,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"22812675-73f6-4c28-b78e-ab1725d6d516"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12637"]},"metadata":{},"execution_count":107}]},{"cell_type":"code","source":["question_sequence = tokenizer.texts_to_sequences(questions)\n","answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n","answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"],"metadata":{"id":"GAAMIHxadKhR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question_padded = pad_sequences(question_sequence, maxlen=30, truncating='post', padding='post')\n","answer_in_padded = pad_sequences(answer_in_sequence, maxlen=30, truncating='post', padding='post')\n","answer_out_padded = pad_sequences(answer_out_sequence, maxlen=30, truncating='post', padding='post')"],"metadata":{"id":"lXYEOQfTdMRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question_padded.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEHVOiNJdPnt","executionInfo":{"status":"ok","timestamp":1689862196689,"user_tz":-540,"elapsed":19,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"dc75e2e0-7470-45f8-ce27-7127ba42903c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11823, 30)"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","source":["print('인코더의 입력의 크기(shape) :',question_padded.shape)\n","print('디코더의 입력의 크기(shape) :',answer_in_padded.shape)\n","print('디코더의 레이블의 크기(shape) :',answer_out_padded.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJSSj2TNTTuB","executionInfo":{"status":"ok","timestamp":1689862196689,"user_tz":-540,"elapsed":18,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"ad1d9fbe-5fc9-4056-abf7-37175f780912"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (11823, 30)\n","디코더의 입력의 크기(shape) : (11823, 30)\n","디코더의 레이블의 크기(shape) : (11823, 30)\n"]}]},{"cell_type":"code","source":["vocab_size = len(tokenizer.word_index) + 1\n","print(\"단어 집합의 크기 : \",vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljoFu1sTTatu","executionInfo":{"status":"ok","timestamp":1689862196689,"user_tz":-540,"elapsed":13,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"2608429c-7ec2-4302-ced3-5b3c7529ab28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합의 크기 :  12638\n"]}]},{"cell_type":"code","source":["word_to_index = tokenizer.word_index\n","index_to_word = tokenizer.index_word"],"metadata":{"id":"oGlB33o3TlaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_to_word[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"UijYicOrTqAk","executionInfo":{"status":"ok","timestamp":1689862196690,"user_tz":-540,"elapsed":12,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"55e17a81-3a9a-4d89-9b5d-ecc9115c6394"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<OOV>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":114}]},{"cell_type":"markdown","source":["# 모형"],"metadata":{"id":"C-ikv8uddSyt"}},{"cell_type":"code","source":["VOCAB_SIZE = len(tokenizer.word_index)+1"],"metadata":{"id":"RoPzzutdkDLU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"o--4AhuwdUmI"}},{"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n","        super(Encoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=time_steps, name='Embedding')\n","        self.dropout = tf.keras.layers.Dropout(0.2, name='Dropout')\n","        # (attention) return_sequences=True 추가\n","        self.lstm = tf.keras.layers.LSTM(units, return_state=True, return_sequences=True, name='LSTM')\n","\n","    def call(self, inputs):\n","        x = self.embedding(inputs)\n","        x = self.dropout(x)\n","        x, hidden_state, cell_state = self.lstm(x)\n","        # (attention) x return 추가\n","        return x, [hidden_state, cell_state]"],"metadata":{"id":"wduCzzC-dT3Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"vMX_kpTEdZ-M"}},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n","        super(Decoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=time_steps, name='Embedding')\n","        self.dropout = tf.keras.layers.Dropout(0.2, name='Dropout')\n","        self.lstm = tf.keras.layers.LSTM(units,\n","                         return_state=True,\n","                         return_sequences=True,\n","                         name='LSTM'\n","                        )\n","        self.attention = tf.keras.layers.Attention(name='Attention')\n","        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax', name='Dense')\n","\n","    def call(self, inputs, initial_state):\n","        # (attention) encoder_inputs 추가\n","        encoder_inputs, decoder_inputs = inputs\n","        x = self.embedding(decoder_inputs)\n","        x = self.dropout(x)\n","        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n","\n","        # (attention) key_value, attention_matrix 추가\n","        # 이전 hidden_state의 값을 concat으로 만들어 vector를 생성합니다.\n","        key_value = tf.concat([initial_state[0][:, tf.newaxis, :], x[:, :-1, :]], axis=1)\n","        # 이전 hidden_state의 값을 concat으로 만든 vector와 encoder에서 나온 출력 값들로 attention을 구합니다.\n","        attention_matrix = self.attention([key_value, encoder_inputs])\n","        # 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n","        x = tf.concat([x, attention_matrix], axis=-1)\n","\n","        x = self.dense(x)\n","        return x, hidden_state, cell_state"],"metadata":{"id":"ziprKe_GdWV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(tf.keras.Model):\n","    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n","        super(Seq2Seq, self).__init__()\n","        self.start_token = start_token\n","        self.end_token = end_token\n","        self.time_steps = time_steps\n","\n","        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n","        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n","\n","\n","    def call(self, inputs,training=False,mask=None):\n","        if training:\n","            encoder_inputs, decoder_inputs = inputs\n","            # (attention) encoder 출력 값 수정\n","            encoder_outputs, context_vector = self.encoder(encoder_inputs)\n","            # (attention) decoder 입력 값 수정\n","            decoder_outputs, _, _ = self.decoder((encoder_outputs, decoder_inputs), initial_state=context_vector)\n","            return decoder_outputs\n","        else:\n","            x = inputs\n","            # (attention) encoder 출력 값 수정\n","            encoder_outputs, context_vector = self.encoder(x)\n","            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n","            results = tf.TensorArray(tf.int32, self.time_steps)\n","\n","            for i in tf.range(self.time_steps):\n","                decoder_output, decoder_hidden, decoder_cell = self.decoder((encoder_outputs, target_seq), initial_state=context_vector)\n","                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n","                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n","                results = results.write(i, decoder_output)\n","\n","                if decoder_output == self.end_token:\n","                    break\n","\n","                target_seq = decoder_output\n","                context_vector = [decoder_hidden, decoder_cell]\n","\n","            return tf.reshape(results.stack(), shape=(1, self.time_steps))"],"metadata":{"id":"u6_kHISvdao7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 변환된 index를 다시 단어로 변환"],"metadata":{"id":"7vDyc6avkSYC"}},{"cell_type":"code","source":["def convert_index_to_text(indexs, end_token):\n","\n","    sentence = ''\n","\n","    # 모든 문장에 대해서 반복\n","    for index in indexs:\n","        if index == end_token:\n","            # 끝 단어이므로 예측 중비\n","            break;\n","        # 사전에 존재하는 단어의 경우 단어 추가\n","        if index > 0 and tokenizer.index_word[index] is not None:\n","            sentence += tokenizer.index_word[index]\n","        else:\n","        # 사전에 없는 인덱스면 빈 문자열 추가\n","            sentence += ''\n","\n","        # 빈칸 추가\n","        sentence += ' '\n","    return sentence"],"metadata":{"id":"kyu0d-GvkT-u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 학습"],"metadata":{"id":"mhcjGxiPUAaA"}},{"cell_type":"code","source":["EMBEDDING_DIM = 100\n","TIME_STEPS = 30\n","START_TOKEN = tokenizer.word_index['<START>']\n","END_TOKEN = tokenizer.word_index['<END>']\n","\n","UNITS = 128\n","\n","VOCAB_SIZE = len(tokenizer.word_index)+1\n","DATA_LENGTH = len(questions)\n","SAMPLE_SIZE = 3"],"metadata":{"id":"p2mdxCPZkXJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Seq2Seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)"],"metadata":{"id":"GwlmKDvGkkPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"],"metadata":{"id":"6ho2hMZLkreM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=3,mode='min',verbose=1)\n","checkpoint_path = 'training_checkpoint.h5'\n","mc = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                             save_weights_only=True,\n","                             save_best_only=True,\n","                             monitor='loss',\n","                             verbose=1\n","                            )"],"metadata":{"id":"cpIFMcXnYoEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit([question_padded, answer_in_padded],answer_out_padded,epochs=350,batch_size=64, callbacks=[mc,es])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9YVdctzmn6J","executionInfo":{"status":"ok","timestamp":1689862616413,"user_tz":-540,"elapsed":405685,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"0ed31e62-36c7-4ed7-d2ea-0d8cc4e89ab0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/350\n","185/185 [==============================] - ETA: 0s - loss: 2.1860 - acc: 0.7870\n","Epoch 1: loss improved from inf to 2.18599, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 27s 100ms/step - loss: 2.1860 - acc: 0.7870\n","Epoch 2/350\n","185/185 [==============================] - ETA: 0s - loss: 1.2823 - acc: 0.8219\n","Epoch 2: loss improved from 2.18599 to 1.28227, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 8s 42ms/step - loss: 1.2823 - acc: 0.8219\n","Epoch 3/350\n","185/185 [==============================] - ETA: 0s - loss: 1.2108 - acc: 0.8268\n","Epoch 3: loss improved from 1.28227 to 1.21081, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 1.2108 - acc: 0.8268\n","Epoch 4/350\n","185/185 [==============================] - ETA: 0s - loss: 1.1667 - acc: 0.8308\n","Epoch 4: loss improved from 1.21081 to 1.16675, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 5s 29ms/step - loss: 1.1667 - acc: 0.8308\n","Epoch 5/350\n","185/185 [==============================] - ETA: 0s - loss: 1.1275 - acc: 0.8340\n","Epoch 5: loss improved from 1.16675 to 1.12749, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 1.1275 - acc: 0.8340\n","Epoch 6/350\n","185/185 [==============================] - ETA: 0s - loss: 1.0860 - acc: 0.8375\n","Epoch 6: loss improved from 1.12749 to 1.08598, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 5s 25ms/step - loss: 1.0860 - acc: 0.8375\n","Epoch 7/350\n","185/185 [==============================] - ETA: 0s - loss: 1.0461 - acc: 0.8420\n","Epoch 7: loss improved from 1.08598 to 1.04612, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 1.0461 - acc: 0.8420\n","Epoch 8/350\n","185/185 [==============================] - ETA: 0s - loss: 1.0099 - acc: 0.8458\n","Epoch 8: loss improved from 1.04612 to 1.00993, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 1.0099 - acc: 0.8458\n","Epoch 9/350\n","185/185 [==============================] - ETA: 0s - loss: 0.9773 - acc: 0.8489\n","Epoch 9: loss improved from 1.00993 to 0.97732, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 23ms/step - loss: 0.9773 - acc: 0.8489\n","Epoch 10/350\n","185/185 [==============================] - ETA: 0s - loss: 0.9473 - acc: 0.8516\n","Epoch 10: loss improved from 0.97732 to 0.94728, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 24ms/step - loss: 0.9473 - acc: 0.8516\n","Epoch 11/350\n","185/185 [==============================] - ETA: 0s - loss: 0.9203 - acc: 0.8544\n","Epoch 11: loss improved from 0.94728 to 0.92031, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 5s 26ms/step - loss: 0.9203 - acc: 0.8544\n","Epoch 12/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8949 - acc: 0.8566\n","Epoch 12: loss improved from 0.92031 to 0.89492, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.8949 - acc: 0.8566\n","Epoch 13/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8714 - acc: 0.8589\n","Epoch 13: loss improved from 0.89492 to 0.87142, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.8714 - acc: 0.8589\n","Epoch 14/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8494 - acc: 0.8608\n","Epoch 14: loss improved from 0.87142 to 0.84942, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.8494 - acc: 0.8608\n","Epoch 15/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8285 - acc: 0.8627\n","Epoch 15: loss improved from 0.84942 to 0.82854, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.8285 - acc: 0.8627\n","Epoch 16/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8083 - acc: 0.8647\n","Epoch 16: loss improved from 0.82854 to 0.80828, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 24ms/step - loss: 0.8083 - acc: 0.8647\n","Epoch 17/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7888 - acc: 0.8665\n","Epoch 17: loss improved from 0.80828 to 0.78877, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.7888 - acc: 0.8665\n","Epoch 18/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7689 - acc: 0.8686\n","Epoch 18: loss improved from 0.78877 to 0.76893, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.7689 - acc: 0.8686\n","Epoch 19/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7494 - acc: 0.8706\n","Epoch 19: loss improved from 0.76893 to 0.74942, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 23ms/step - loss: 0.7494 - acc: 0.8706\n","Epoch 20/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7301 - acc: 0.8724\n","Epoch 20: loss improved from 0.74942 to 0.73009, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.7301 - acc: 0.8724\n","Epoch 21/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7114 - acc: 0.8747\n","Epoch 21: loss improved from 0.73009 to 0.71136, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.7114 - acc: 0.8747\n","Epoch 22/350\n","185/185 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.8767\n","Epoch 22: loss improved from 0.71136 to 0.69368, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6937 - acc: 0.8767\n","Epoch 23/350\n","185/185 [==============================] - ETA: 0s - loss: 0.6778 - acc: 0.8787\n","Epoch 23: loss improved from 0.69368 to 0.67779, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6778 - acc: 0.8787\n","Epoch 24/350\n","185/185 [==============================] - ETA: 0s - loss: 0.6595 - acc: 0.8809\n","Epoch 24: loss improved from 0.67779 to 0.65947, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.6595 - acc: 0.8809\n","Epoch 25/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.6422 - acc: 0.8834\n","Epoch 25: loss improved from 0.65947 to 0.64199, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6420 - acc: 0.8835\n","Epoch 26/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8855\n","Epoch 26: loss improved from 0.64199 to 0.62499, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6250 - acc: 0.8856\n","Epoch 27/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.8879\n","Epoch 27: loss improved from 0.62499 to 0.60893, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.6089 - acc: 0.8880\n","Epoch 28/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5920 - acc: 0.8907\n","Epoch 28: loss improved from 0.60893 to 0.59204, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.5920 - acc: 0.8907\n","Epoch 29/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5768 - acc: 0.8933\n","Epoch 29: loss improved from 0.59204 to 0.57680, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5768 - acc: 0.8933\n","Epoch 30/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8955\n","Epoch 30: loss improved from 0.57680 to 0.56187, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.5619 - acc: 0.8955\n","Epoch 31/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5452 - acc: 0.8979\n","Epoch 31: loss improved from 0.56187 to 0.54519, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5452 - acc: 0.8979\n","Epoch 32/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.9005\n","Epoch 32: loss improved from 0.54519 to 0.53039, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5304 - acc: 0.9004\n","Epoch 33/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5163 - acc: 0.9026\n","Epoch 33: loss improved from 0.53039 to 0.51626, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5163 - acc: 0.9026\n","Epoch 34/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.9051\n","Epoch 34: loss improved from 0.51626 to 0.50212, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.5021 - acc: 0.9051\n","Epoch 35/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.9070\n","Epoch 35: loss improved from 0.50212 to 0.48906, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.4891 - acc: 0.9070\n","Epoch 36/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4765 - acc: 0.9093\n","Epoch 36: loss improved from 0.48906 to 0.47650, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4765 - acc: 0.9093\n","Epoch 37/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.9114\n","Epoch 37: loss improved from 0.47650 to 0.46468, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.4647 - acc: 0.9113\n","Epoch 38/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.9135\n","Epoch 38: loss improved from 0.46468 to 0.45333, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.4533 - acc: 0.9134\n","Epoch 39/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.9152\n","Epoch 39: loss improved from 0.45333 to 0.44180, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.4418 - acc: 0.9152\n","Epoch 40/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.9168\n","Epoch 40: loss improved from 0.44180 to 0.43026, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.4303 - acc: 0.9168\n","Epoch 41/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4199 - acc: 0.9188\n","Epoch 41: loss improved from 0.43026 to 0.41992, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.4199 - acc: 0.9188\n","Epoch 42/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.9203\n","Epoch 42: loss improved from 0.41992 to 0.40991, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.4099 - acc: 0.9203\n","Epoch 43/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4006 - acc: 0.9220\n","Epoch 43: loss improved from 0.40991 to 0.40061, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4006 - acc: 0.9220\n","Epoch 44/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3903 - acc: 0.9237\n","Epoch 44: loss improved from 0.40061 to 0.39033, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3903 - acc: 0.9237\n","Epoch 45/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3820 - acc: 0.9253\n","Epoch 45: loss improved from 0.39033 to 0.38196, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.3820 - acc: 0.9253\n","Epoch 46/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.9269\n","Epoch 46: loss improved from 0.38196 to 0.37236, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.3724 - acc: 0.9269\n","Epoch 47/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3643 - acc: 0.9279\n","Epoch 47: loss improved from 0.37236 to 0.36425, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3643 - acc: 0.9279\n","Epoch 48/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3552 - acc: 0.9294\n","Epoch 48: loss improved from 0.36425 to 0.35524, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3552 - acc: 0.9294\n","Epoch 49/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3475 - acc: 0.9310\n","Epoch 49: loss improved from 0.35524 to 0.34753, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3475 - acc: 0.9310\n","Epoch 50/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3396 - acc: 0.9324\n","Epoch 50: loss improved from 0.34753 to 0.33957, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3396 - acc: 0.9324\n","Epoch 51/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3331 - acc: 0.9334\n","Epoch 51: loss improved from 0.33957 to 0.33305, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3331 - acc: 0.9334\n","Epoch 52/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.9345\n","Epoch 52: loss improved from 0.33305 to 0.32523, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.3252 - acc: 0.9345\n","Epoch 53/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9358\n","Epoch 53: loss improved from 0.32523 to 0.31775, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.3178 - acc: 0.9358\n","Epoch 54/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3111 - acc: 0.9367\n","Epoch 54: loss improved from 0.31775 to 0.31106, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 23ms/step - loss: 0.3111 - acc: 0.9367\n","Epoch 55/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9384\n","Epoch 55: loss improved from 0.31106 to 0.30317, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.3032 - acc: 0.9384\n","Epoch 56/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9392\n","Epoch 56: loss improved from 0.30317 to 0.29729, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2973 - acc: 0.9392\n","Epoch 57/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2913 - acc: 0.9403\n","Epoch 57: loss improved from 0.29729 to 0.29129, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2913 - acc: 0.9403\n","Epoch 58/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9415\n","Epoch 58: loss improved from 0.29129 to 0.28458, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2846 - acc: 0.9415\n","Epoch 59/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2792 - acc: 0.9424\n","Epoch 59: loss improved from 0.28458 to 0.27920, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2792 - acc: 0.9424\n","Epoch 60/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2723 - acc: 0.9436\n","Epoch 60: loss improved from 0.27920 to 0.27230, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2723 - acc: 0.9436\n","Epoch 61/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9442\n","Epoch 61: loss improved from 0.27230 to 0.26846, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.2685 - acc: 0.9442\n","Epoch 62/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9453\n","Epoch 62: loss improved from 0.26846 to 0.26146, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.2615 - acc: 0.9453\n","Epoch 63/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9465\n","Epoch 63: loss improved from 0.26146 to 0.25540, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2554 - acc: 0.9465\n","Epoch 64/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9472\n","Epoch 64: loss improved from 0.25540 to 0.24984, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.2498 - acc: 0.9472\n","Epoch 65/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9481\n","Epoch 65: loss improved from 0.24984 to 0.24385, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.2438 - acc: 0.9481\n","Epoch 66/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9492\n","Epoch 66: loss improved from 0.24385 to 0.23883, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.2388 - acc: 0.9492\n","Epoch 67/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9504\n","Epoch 67: loss improved from 0.23883 to 0.23271, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 3s 19ms/step - loss: 0.2327 - acc: 0.9505\n","Epoch 68/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2277 - acc: 0.9513\n","Epoch 68: loss improved from 0.23271 to 0.22772, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2277 - acc: 0.9513\n","Epoch 69/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9521\n","Epoch 69: loss improved from 0.22772 to 0.22283, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.2228 - acc: 0.9521\n","Epoch 70/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9531\n","Epoch 70: loss improved from 0.22283 to 0.21747, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2175 - acc: 0.9531\n","Epoch 71/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9540\n","Epoch 71: loss improved from 0.21747 to 0.21266, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.2127 - acc: 0.9540\n","Epoch 72/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9546\n","Epoch 72: loss improved from 0.21266 to 0.20807, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.2081 - acc: 0.9546\n","Epoch 73/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2026 - acc: 0.9560\n","Epoch 73: loss improved from 0.20807 to 0.20257, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 23ms/step - loss: 0.2026 - acc: 0.9560\n","Epoch 74/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9564\n","Epoch 74: loss improved from 0.20257 to 0.19974, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1997 - acc: 0.9563\n","Epoch 75/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9572\n","Epoch 75: loss improved from 0.19974 to 0.19437, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1944 - acc: 0.9572\n","Epoch 76/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9579\n","Epoch 76: loss improved from 0.19437 to 0.18962, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1896 - acc: 0.9579\n","Epoch 77/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9588\n","Epoch 77: loss improved from 0.18962 to 0.18652, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1865 - acc: 0.9589\n","Epoch 78/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1820 - acc: 0.9599\n","Epoch 78: loss improved from 0.18652 to 0.18199, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1820 - acc: 0.9599\n","Epoch 79/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9608\n","Epoch 79: loss improved from 0.18199 to 0.17702, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1770 - acc: 0.9608\n","Epoch 80/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9614\n","Epoch 80: loss improved from 0.17702 to 0.17355, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1735 - acc: 0.9614\n","Epoch 81/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1714 - acc: 0.9614\n","Epoch 81: loss improved from 0.17355 to 0.17141, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1714 - acc: 0.9614\n","Epoch 82/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1657 - acc: 0.9629\n","Epoch 82: loss improved from 0.17141 to 0.16565, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 22ms/step - loss: 0.1657 - acc: 0.9629\n","Epoch 83/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9634\n","Epoch 83: loss improved from 0.16565 to 0.16329, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1633 - acc: 0.9635\n","Epoch 84/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9646\n","Epoch 84: loss improved from 0.16329 to 0.15839, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1584 - acc: 0.9646\n","Epoch 85/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9650\n","Epoch 85: loss improved from 0.15839 to 0.15524, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1552 - acc: 0.9650\n","Epoch 86/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9659\n","Epoch 86: loss improved from 0.15524 to 0.15059, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1506 - acc: 0.9659\n","Epoch 87/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1474 - acc: 0.9669\n","Epoch 87: loss improved from 0.15059 to 0.14739, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1474 - acc: 0.9669\n","Epoch 88/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9674\n","Epoch 88: loss improved from 0.14739 to 0.14404, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1440 - acc: 0.9673\n","Epoch 89/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1403 - acc: 0.9681\n","Epoch 89: loss improved from 0.14404 to 0.14034, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1403 - acc: 0.9681\n","Epoch 90/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9692\n","Epoch 90: loss improved from 0.14034 to 0.13680, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1368 - acc: 0.9692\n","Epoch 91/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9702\n","Epoch 91: loss improved from 0.13680 to 0.13284, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1328 - acc: 0.9702\n","Epoch 92/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9697\n","Epoch 92: loss did not improve from 0.13284\n","185/185 [==============================] - 3s 18ms/step - loss: 0.1336 - acc: 0.9696\n","Epoch 93/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9712\n","Epoch 93: loss improved from 0.13284 to 0.12841, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1284 - acc: 0.9712\n","Epoch 94/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9714\n","Epoch 94: loss improved from 0.12841 to 0.12589, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1259 - acc: 0.9714\n","Epoch 95/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9721\n","Epoch 95: loss improved from 0.12589 to 0.12257, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1226 - acc: 0.9721\n","Epoch 96/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1198 - acc: 0.9725\n","Epoch 96: loss improved from 0.12257 to 0.11977, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1198 - acc: 0.9725\n","Epoch 97/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9735\n","Epoch 97: loss improved from 0.11977 to 0.11695, saving model to training_checkpoint.ckpt\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1170 - acc: 0.9735\n","Epoch 98/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1370 - acc: 0.9692\n","Epoch 98: loss did not improve from 0.11695\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1370 - acc: 0.9692\n","Epoch 99/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9663\n","Epoch 99: loss did not improve from 0.11695\n","185/185 [==============================] - 3s 18ms/step - loss: 0.1446 - acc: 0.9663\n","Epoch 100/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1230 - acc: 0.9716\n","Epoch 100: loss did not improve from 0.11695\n","185/185 [==============================] - 4s 19ms/step - loss: 0.1230 - acc: 0.9716\n","Epoch 100: early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fc708daf730>"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","source":["def make_prediction(model, question_inputs):\n","    results = model(inputs=question_inputs, training=False)\n","    # 변환된 인덱스를 문장으로 변환\n","    results = np.asarray(results).reshape(-1)\n","    return results"],"metadata":{"id":"_i7kTyGxoYYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 자연어 (질문 입력) 대한 전처리 함수\n","def make_question(sentence):\n","    sentence = clean_and_morph(sentence)\n","    question_sequence = tokenizer.texts_to_sequences([sentence])\n","    question_padded = pad_sequences(question_sequence, maxlen=30, truncating='post', padding='post')\n","    return question_padded"],"metadata":{"id":"KlOFjmEGp7yo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_chatbot(question):\n","    question_inputs = make_question(question)\n","    results = make_prediction(model, question_inputs)\n","    results = convert_index_to_text(results, END_TOKEN)\n","    return results"],"metadata":{"id":"6zW3xy8Tp_xI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    user_input = input('<< 말을 걸어 보세요!\\n')\n","    if user_input == 'q':\n","        break\n","    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiiAyWUuqDW6","executionInfo":{"status":"ok","timestamp":1689862812460,"user_tz":-540,"elapsed":8473,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"27030204-2306-4ac7-9df7-ea15f575890e"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["<< 말을 걸어 보세요!\n","커피를 마시고 싶습니다\n",">> 챗봇 응답: 카페인 이 필요한 시간 인가 봐요 \n","<< 말을 걸어 보세요!\n","q\n"]}]},{"cell_type":"code","source":["model.save_weights('training_checkpoint.h5')"],"metadata":{"id":"zEkKsT1YqjAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_weights('training_checkpoint.h5')"],"metadata":{"id":"vLe6K86Vq5Mm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vyK9OsWq_JN","executionInfo":{"status":"ok","timestamp":1689862834549,"user_tz":-540,"elapsed":465,"user":{"displayName":"이동호","userId":"13068063960271114746"}},"outputId":"96bcb340-485c-4221-f65a-3f899891ee23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul 20 14:20:34 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    40W / 300W |   2990MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VutYg2bPrIIY"},"execution_count":null,"outputs":[]}]}