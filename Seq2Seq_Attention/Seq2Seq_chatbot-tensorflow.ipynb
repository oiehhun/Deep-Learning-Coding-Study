{"cells":[{"cell_type":"markdown","metadata":{},"source":["- 기계번역\n","- 챗봇\n","\n","요즘에는 많이 사용하지 않음 <- Transformer가 나와서\n","RNN은 시계열 쪽에서 사용"]},{"cell_type":"markdown","metadata":{"id":"OHTeQfQZnUqp"},"source":["# Module"]},{"cell_type":"markdown","metadata":{},"source":["- 기계번역\n","- 챗봇\n","\n","요즘에는 많이 사용하지 않음 <- Transformer가 나와서"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689938577140,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"K3xsZAsbnHDL"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-08-03 10:32:44.978247: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-08-03 10:32:45.038327: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-08-03 10:32:45.039979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-03 10:32:46.063467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import re\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"markdown","metadata":{"id":"4bpa6jOlZSEZ"},"source":["# Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689931346015,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"B0UJiJzFZJs7","outputId":"2cb699d5-29cc-4ab2-a96f-571d90ff459e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q</th>\n","      <th>A</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12시 땡!</td>\n","      <td>하루가 또 가네요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1지망 학교 떨어졌어</td>\n","      <td>위로해 드립니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3박4일 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3박4일 정도 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PPL 심하네</td>\n","      <td>눈살이 찌푸려지죠.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Q            A  label\n","0           12시 땡!   하루가 또 가네요.      0\n","1      1지망 학교 떨어졌어    위로해 드립니다.      0\n","2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n","3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n","4          PPL 심하네   눈살이 찌푸려지죠.      0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n","df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1689931346015,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"wJRim1ydZ-If","outputId":"a00b249b-7322-4f9a-b93e-13e143a4fa64"},"outputs":[{"data":{"text/plain":["(11823, 3)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1689931346015,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"EXEDf1HeZ-7g","outputId":"716ada84-e6e1-4433-dfe0-2a6fad5e499a"},"outputs":[{"data":{"text/plain":["Q        0\n","A        0\n","label    0\n","dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"T5qybW2VZrFY"},"source":["# Data preprocessing"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":454,"status":"ok","timestamp":1689934239541,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"_lB-Zs2qZTmk"},"outputs":[],"source":["texts = []\n","pairs = []\n","\n","for i in range(len(df)):\n","  texts.append(df.iloc[i,0])\n","  pairs.append(df.iloc[i,1])"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689934239541,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"eTBu_vp4Z4wb"},"outputs":[],"source":["# 특수 문자 제거\n","def clean_sentence(sentence):\n","    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n","    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n","    return sentence"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3607,"status":"ok","timestamp":1689934243145,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"Y83-mAKFaEvE","outputId":"7a401cf8-f288-4ca0-cada-0d281ac7c79a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting konlpy\n","  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Using cached JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","Collecting lxml>=4.1.0 (from konlpy)\n","  Downloading lxml-4.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /home/xogns5037/.conda/envs/th/lib/python3.8/site-packages (from konlpy) (1.24.3)\n","Requirement already satisfied: packaging in /home/xogns5037/.conda/envs/th/lib/python3.8/site-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","Installing collected packages: lxml, JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0 lxml-4.9.3\n"]}],"source":["!pip install konlpy # 코랩에서는 항상 설치 해줘야함"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689934243146,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"e7edknXjaKmh"},"outputs":[],"source":["from konlpy.tag import Okt\n","okt = Okt()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689934243146,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"a5oh2tQkaPox"},"outputs":[],"source":["# 형태소 반환\n","def process_morph(sentence):\n","    return ' '.join(okt.morphs(sentence)) # morphs: 형태소 분석 -> 리스트 반환"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'1지망 학교 떨어졌어'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["texts[1]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["['1', '지망', '학교', '떨어졌어']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["okt.morphs(texts[1])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["'1 지망 학교 떨어졌어'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["' '.join(okt.morphs(texts[1]))"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689934243147,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"rrcjbpWFaSzr"},"outputs":[],"source":["def clean_and_morph(sentence, is_question=True):\n","    # 한글 문장 전처리\n","    sentence = clean_sentence(sentence)\n","    # 형태소 변환\n","    sentence = process_morph(sentence)\n","    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n","    if is_question:\n","        return sentence\n","    else:\n","        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n","        return ('<START> ' + sentence, sentence + ' <END>') # <START> : <SOS>, <END> : <EOS>"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["'위로해 드립니다.'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["s = pairs[1]\n","s"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["('<START> 위로해 드립니다.', '위로해 드립니다. <END>')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["'<START> ' + s, s + ' <END>'"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689934243147,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"X_fRDhxmaVFp"},"outputs":[],"source":["def preprocess(texts, pairs):\n","    questions = [] # encoder input\n","    answer_in = [] # decoder input\n","    answer_out = [] # decoder output\n","\n","    # 질의에 대한 전처리\n","    for text in texts:\n","        # 전처리와 morph 수행\n","        question = clean_and_morph(text, is_question=True)\n","        questions.append(question)\n","\n","    # 답변에 대한 전처리\n","    for pair in pairs:\n","        # 전처리와 morph 수행\n","        in_, out_ = clean_and_morph(pair, is_question=False)\n","        answer_in.append(in_)\n","        answer_out.append(out_)\n","\n","    return questions, answer_in, answer_out"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":31975,"status":"ok","timestamp":1689934275117,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"heCCw66FaWgm"},"outputs":[],"source":["questions, answer_in, answer_out = preprocess(texts, pairs)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1689934275118,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"c-wwRAUIaXtK","outputId":"b56c0e78-6a4c-489a-f6e1-517c610efc7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', '심하네']\n"]}],"source":["print(questions[:5]) # encoder_input"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1689934275118,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"XITW_z-Xae85","outputId":"d58b844c-70a8-4aca-ba2e-c6ad2a54f9de"},"outputs":[{"name":"stdout","output_type":"stream","text":["['<START> 하루 가 또 가네요', '<START> 위로 해 드립니다', '<START> 여행 은 언제나 좋죠', '<START> 여행 은 언제나 좋죠', '<START> 눈살 이 찌푸려지죠']\n"]}],"source":["print(answer_in[:5]) # decoder input"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689934275119,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"7TwYr3m_agH8","outputId":"73924b3c-3508-4e69-9920-8c12064bd554"},"outputs":[{"name":"stdout","output_type":"stream","text":["['하루 가 또 가네요 <END>', '위로 해 드립니다 <END>', '여행 은 언제나 좋죠 <END>', '여행 은 언제나 좋죠 <END>', '눈살 이 찌푸려지죠 <END>']\n"]}],"source":["print(answer_out[:5]) # decoder output"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689934275120,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"oZqgN18CamfG"},"outputs":[],"source":["all_sentences = questions + answer_in + answer_out"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Tokenizer\n","tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>') \n","# filters='' : 특수문자를 제거 안함\n","# lower=False : 대문자를 소문자로 변환 안함\n","# oov_token : out of vocabulary"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":474,"status":"ok","timestamp":1689934275589,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"5PKdSe4UbI4l"},"outputs":[],"source":["tokenizer.fit_on_texts(all_sentences)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689934275589,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"0MNinHlEbKDa","outputId":"c98e95d8-fbf9-48bd-a228-b8134c3e07bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["<OOV>\t\t => \t1\n","<START>\t\t => \t2\n","<END>\t\t => \t3\n","이\t\t => \t4\n","을\t\t => \t5\n","거\t\t => \t6\n","가\t\t => \t7\n","예요\t\t => \t8\n","사람\t\t => \t9\n","요\t\t => \t10\n","에\t\t => \t11\n"]}],"source":["for word, idx in tokenizer.word_index.items():\n","    print(f'{word}\\t\\t => \\t{idx}')\n","    if idx > 10:\n","        break"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689934275589,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"ZWfTVPRwbOqv","outputId":"9070a39c-e836-4f40-df83-5a55422a914d"},"outputs":[{"data":{"text/plain":["12637"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(tokenizer.word_index)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689934275590,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"GAAMIHxadKhR"},"outputs":[],"source":["question_sequence = tokenizer.texts_to_sequences(questions)\n","answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n","answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":448,"status":"ok","timestamp":1689934276033,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"lXYEOQfTdMRF"},"outputs":[],"source":["question_padded = pad_sequences(question_sequence, maxlen=30, truncating='post', padding='post')\n","answer_in_padded = pad_sequences(answer_in_sequence, maxlen=30, truncating='post', padding='post')\n","answer_out_padded = pad_sequences(answer_out_sequence, maxlen=30, truncating='post', padding='post')"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1689934276033,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"UEHVOiNJdPnt","outputId":"4b489ebc-e6a9-4665-9117-9c2a0eb1ae41"},"outputs":[{"data":{"text/plain":["(11823, 30)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["question_padded.shape"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1689934276033,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"kJSSj2TNTTuB","outputId":"9fd28a34-1dd3-4ad2-e142-467d80730da8"},"outputs":[{"name":"stdout","output_type":"stream","text":["인코더의 입력의 크기(shape) : (11823, 30)\n","디코더의 입력의 크기(shape) : (11823, 30)\n","디코더의 레이블의 크기(shape) : (11823, 30)\n"]}],"source":["print('인코더의 입력의 크기(shape) :',question_padded.shape)\n","print('디코더의 입력의 크기(shape) :',answer_in_padded.shape)\n","print('디코더의 레이블의 크기(shape) :',answer_out_padded.shape)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1689934276034,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"ljoFu1sTTatu","outputId":"b8c78d47-ae2d-45bd-88f5-4aa5c9bbad80"},"outputs":[{"name":"stdout","output_type":"stream","text":["단어 집합의 크기 :  12638\n"]}],"source":["vocab_size = len(tokenizer.word_index) + 1 # +1 : 0번 패딩 토큰을 고려하여 +1\n","print(\"단어 집합의 크기 : \",vocab_size)"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1689934276034,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"oGlB33o3TlaM"},"outputs":[],"source":["word_to_index = tokenizer.word_index\n","index_to_word = tokenizer.index_word"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1689934276035,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"UijYicOrTqAk","outputId":"f2b34cf1-5526-49f8-be33-f4fdf041922a"},"outputs":[{"data":{"text/plain":["'<OOV>'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["index_to_word[1]"]},{"cell_type":"markdown","metadata":{"id":"C-ikv8uddSyt"},"source":["# 모형"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1689934276036,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"TVT-G8hfw5S2"},"outputs":[],"source":["VOCAB_SIZE = len(tokenizer.word_index)+1"]},{"cell_type":"markdown","metadata":{"id":"o--4AhuwdUmI"},"source":["## Encoder"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# # 실습용 코드\n","# class Encoder(tf.keras.Model): # tf.keras.Model / tf.keras.layers.Layer\n","\n","#     def __init__(self, vocab_size, embedding_dim, hidden_unit, time_steps):\n","#         super().__init__()\n","#         self.vocab_size = vocab_size\n","#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=time_steps)\n","#         self.dropout = tf.keras.layers.Dropout(0.5)\n","#         self.lstm = tf.keras.layers.LSTM(hidden_unit, retrurn_sequences=True, return_state=True)\n","\n","#     def call(self, x, training=None, mask=None):\n","#         x = self.embedding(x)\n","#         x = self.dropout(x)\n","#         x, h, c = self.lstm(x)\n","#         return h, c"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# enc = Encoder(vocab_size, 30, 20, 30)\n","# enc(question_padded[0].reshape(1, -1))"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1689934276036,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"wduCzzC-dT3Q"},"outputs":[],"source":["class Encoder(tf.keras.Model):\n","\n","  # hidden_unit : 은닉층의 노드 수, vocab_size : 단어 사전의 크기, embedding_dim : 임베딩 벡터의 차원 수, time_steps : 입력 시퀀스의 길이\n","  def __init__(self, units, vocab_size, embedding_dim, time_steps): \n","    super().__init__()\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=time_steps)\n","    self.dropout = tf.keras.layers.Dropout(0.2)\n","    self.lstm = tf.keras.layers.LSTM(units, return_state=True, return_sequences=True)\n","\n","\n","  def call(self,inputs):\n","\n","    x = self.embedding(inputs)\n","    x = self.dropout(x)\n","    x, hidden_state, cell_state = self.lstm(x)\n","\n","    return hidden_state, cell_state"]},{"cell_type":"markdown","metadata":{"id":"JyxPZkg4xcWH"},"source":["Tensorflow에서는 return state = True로 해주어야 hidden state와 cell state가 출력이 됩니다. return sequences = True를 해주게 되면 각 time step마다의 출력값을 내보내주게 됩니다."]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["array([5962, 8808,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["question_padded[0]"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1689934276036,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"WS5_ljHrxlbH","outputId":"61b22671-95ce-4ba3-b2b0-921f4afe5efa"},"outputs":[{"data":{"text/plain":["(1, 30)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["sample = question_padded[0].reshape(1,-1) # batch size를 고려해서 1로 reshape\n","sample.shape"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1689934276037,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"t-A68fY3xuoI","outputId":"d060028c-df48-4f48-84a2-31ef28619be8"},"outputs":[{"data":{"text/plain":["TensorShape([1, 30, 100])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["emb = tf.keras.layers.Embedding(VOCAB_SIZE,100,input_length=30)(sample)\n","emb.shape"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1689934276037,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"uVfcveejx7wK"},"outputs":[],"source":["drop = tf.keras.layers.Dropout(0.2)(emb)"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1689934276037,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"60SkewNEyA-B","outputId":"866278e0-901a-41fd-d988-455ff631c109"},"outputs":[{"data":{"text/plain":["TensorShape([1, 128])"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# 아무런 인자를 안 주었을 경우\n","x = tf.keras.layers.LSTM(128)(drop)\n","x.shape"]},{"cell_type":"markdown","metadata":{"id":"K8RmMwH-yQhw"},"source":["가장 마지막 time step의 출력값만을 보여준"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":445,"status":"ok","timestamp":1689934276470,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"-EviE2vpyUHr","outputId":"4aa91d68-7050-48cc-db04-ada196773cf4"},"outputs":[{"data":{"text/plain":["TensorShape([1, 30, 128])"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["# return sequence = True\n","x = tf.keras.layers.LSTM(128,return_sequences=True)(drop)\n","x.shape"]},{"cell_type":"markdown","metadata":{"id":"g6tA-DVqybki"},"source":["각각의 time step마다의 출력값을 보여준다"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1689934276470,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"VZBm1BwByfd7","outputId":"c132c5f0-b43e-4a90-bada-65da2506c6e4"},"outputs":[{"data":{"text/plain":["(TensorShape([1, 30, 128]), TensorShape([1, 128]), TensorShape([1, 128]))"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# return state = True, return sequences = True\n","x,h,c = tf.keras.layers.LSTM(128,return_sequences=True,return_state=True)(drop)\n","x.shape,h.shape, c.shape"]},{"cell_type":"markdown","metadata":{"id":"55pGGNaCypNk"},"source":["가장 마지막 층의 은닉 상태와 cell state도 같이 출력을 해주게 된다"]},{"cell_type":"markdown","metadata":{"id":"vMX_kpTEdZ-M"},"source":["## Decoder"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1689934276470,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"ziprKe_GdWV6"},"outputs":[],"source":["class Decoder(tf.keras.Model):\n","\n","  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n","    super().__init__()\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=time_steps)\n","    self.dropout = tf.keras.layers.Dropout(0.2)\n","    self.lstm = tf.keras.layers.LSTM(units,return_sequences=True,return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n","\n","  def call(self, x, hidden, cell):\n","    x = self.embedding(x)\n","    x = self.dropout(x)\n","    # Encoder의 context vector 반영\n","    x,h,c = self.lstm(x,initial_state=[hidden,cell])\n","\n","    x = self.dense(x)\n","    return x,h,c"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1689934276470,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"-FVm7o7d0dn7","outputId":"63d51276-8cb7-4289-938f-688ded361575"},"outputs":[{"data":{"text/plain":["(1, 30)"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["sample2 = answer_in_padded[0].reshape(1,-1)\n","sample2.shape"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1689934276471,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"GPQ0v3ze0nyw","outputId":"c0929743-6afd-46fd-a600-ca7a68545122"},"outputs":[{"data":{"text/plain":["TensorShape([1, 30, 100])"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["emb2 = tf.keras.layers.Embedding(VOCAB_SIZE,100,input_length=30)(sample2)\n","emb2.shape"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689934276471,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"1Duq9lNl0xdl","outputId":"8d75cbc3-0c6d-49a6-92a9-791d69163f20"},"outputs":[{"data":{"text/plain":["(TensorShape([1, 30, 128]), TensorShape([1, 128]), TensorShape([1, 128]))"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["x2,h2,c2 = tf.keras.layers.LSTM(128,return_sequences=True,return_state=True)(emb2,initial_state=[h,c])\n","x2.shape,h2.shape,c2.shape"]},{"cell_type":"markdown","metadata":{"id":"ePMiuI2e1-lv"},"source":["## Seq2Seq"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938655753,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"HyCGKDNY1_95"},"outputs":[],"source":["class Seq2Seq(tf.keras.Model):\n","\n","  def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n","    super().__init__()\n","    self.start = start_token # SOS token\n","    self.end = end_token # EOS token\n","    self.time_steps = time_steps\n","\n","    self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n","    self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n","\n","  def call(self, x, training=False, mask=None):\n","\n","\n","    # 학습\n","    if training:\n","      enc_input, dec_input = x\n","      hidden, cell = self.encoder(enc_input)\n","      outputs,_,_ = self.decoder(dec_input, hidden, cell)\n","      return outputs\n","\n","    # 추론\n","    else:\n","      hidden, cell = self.encoder(x)\n","      # SOS token을 가장 먼저 넣어줌\n","      target_seq = tf.constant([[self.start]],dtype=tf.float32) # constant : 상수 취급(학습을 안하겠다)\n","      # 결과값을 담는 tensor\n","      results = tf.TensorArray(tf.int32,self.time_steps) # TensorArray : 텐서를 담는 배열(아직 메모리상에 올라가지 않음) -> 메모리를 효율적으로 사용 가능\n","\n","      for i in tf.range(self.time_steps):\n","        output, decoder_hidden, decoder_cell = self.decoder(target_seq, hidden, cell)\n","        output = tf.cast(tf.argmax(output, axis=-1), dtype=tf.int32) # cast = 넘파이의 astype : 자료형 변환 / argmax : 최대값의 인덱스를 반환 -> 굳이 정수형으로 바꿔줘야하나?\n","        output = tf.reshape(output, shape=(1,1))\n","        results = results.write(i,output)\n","\n","        # EOS token을 만나면 문장 생성 중지\n","        if output == self.end:\n","          break\n","\n","        # 이전 값들 최신화\n","        target_seq = output\n","        hidden = decoder_hidden\n","        cell = decoder_cell\n","      return tf.reshape(results.stack(), shape=(1,self.time_steps))\n"]},{"cell_type":"markdown","metadata":{"id":"lis9wFka4bjD"},"source":["### 학습 시"]},{"cell_type":"code","execution_count":201,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689938656374,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"vmWCEogL4czN","outputId":"f5f13557-a5d1-4829-f8e2-a2f2d1f2cf6f"},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7da7ed17e740>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 72, in error_handler\n","    del filtered_tb  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1105, in __call__\n","    with call_context.enter(  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 158, in error_handler\n","    del bound_signature  File \"<ipython-input-179-cf15b3f73bb2>\", line 44, in call\n","    return tf.reshape(results.stack(), shape=(1,self.time_steps))  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 243, in wrapped\n","    return _add_should_use_warning(fn(*args, **kwargs),\n","==================================\n"]}],"source":["encoder = Encoder(128,VOCAB_SIZE,100,30)\n","decoder = Decoder(128,VOCAB_SIZE,100,30)"]},{"cell_type":"code","execution_count":202,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689938656890,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"TlpjRBib4qHl","outputId":"23d6608b-c1f4-4045-e91c-cfa0c07fc060"},"outputs":[{"data":{"text/plain":["(TensorShape([1, 128]), TensorShape([1, 128]))"]},"execution_count":202,"metadata":{},"output_type":"execute_result"}],"source":["hidden, cell = encoder(sample)\n","hidden.shape, cell.shape"]},{"cell_type":"code","execution_count":203,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689938656891,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"u6_kHISvdao7","outputId":"d5d2b652-444d-48d0-d357-cd1ab15799f0"},"outputs":[{"data":{"text/plain":["TensorShape([1, 30, 12640])"]},"execution_count":203,"metadata":{},"output_type":"execute_result"}],"source":["output, _, _ = decoder(sample2, hidden, cell)\n","output.shape"]},{"cell_type":"markdown","metadata":{"id":"sHBVHhvC415F"},"source":["### 추론 시"]},{"cell_type":"code","execution_count":204,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938657314,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"UgT0R8p042-c","outputId":"6ad679f9-5b32-4028-acfb-c7f365d92cfe"},"outputs":[{"data":{"text/plain":["(TensorShape([1, 128]), TensorShape([1, 128]))"]},"execution_count":204,"metadata":{},"output_type":"execute_result"}],"source":["hidden, cell = encoder(sample)\n","hidden.shape, cell.shape"]},{"cell_type":"code","execution_count":205,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689938658134,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"8-hMvTV048l3","outputId":"538cb8ab-a38d-4f3a-d57f-cff5ed763e2a"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":205,"metadata":{},"output_type":"execute_result"}],"source":["start = tokenizer.word_index['<START>']\n","start"]},{"cell_type":"code","execution_count":206,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689938658551,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"fVU3_wpV5E-5","outputId":"cc108298-5a76-4b23-fca6-e16b3b2be627"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 1)\n","tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"]}],"source":["target_seq = tf.constant([[start]],dtype=tf.float32)\n","print(target_seq.shape)\n","print(target_seq)"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":5,"status":"error","timestamp":1689938659110,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"luMrX3P9T5Vi","outputId":"3a1b3df3-c5f8-4703-f1f6-a4e4772e60d7"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-207-53199bd6da83>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 동적 크기 배정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, index, name)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[0;32m-> 1158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf_should_use\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_use_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    784\u001b[0m             index)\n\u001b[1;32m    785\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_after_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36m_maybe_zero\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m       val = self._tensor_array[ix] = array_ops.zeros(\n\u001b[0m\u001b[1;32m    853\u001b[0m           shape=self._element_shape, dtype=self._dtype)\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[0;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    361\u001b[0m         f\"Cannot convert a partially known TensorShape {s} to a Tensor.\")\n\u001b[1;32m    362\u001b[0m   \u001b[0ms_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot convert a partially known TensorShape <unknown> to a Tensor."]}],"source":["# 동적 크기 배정\n","results = tf.TensorArray(tf.int32,30)\n","results.read(0)"]},{"cell_type":"markdown","metadata":{"id":"Xhf7sQi26olA"},"source":["tf.TensorArray는 동적인 객체인데 현재 아무 값도 넣어주지 않았기 때문에 오류가 발생하게 된다"]},{"cell_type":"code","execution_count":208,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689938661689,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"xqfDns9e5rMm","outputId":"4706abd5-1d4a-4afc-c56f-9c40e601f061"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[9150]], dtype=int32)>"]},"execution_count":208,"metadata":{},"output_type":"execute_result"}],"source":["output, decoder_hidden, decoder_cell = decoder(target_seq,hidden,cell)\n","# 가장 확률이 높은 단어를 고르고 tf.cast를 통해 형 변환\n","output = tf.cast(tf.argmax(output,axis=-1),dtype=tf.int32)\n","output"]},{"cell_type":"code","execution_count":209,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689938662248,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"9f7snbKC6Ast","outputId":"c57f250b-56db-4f2a-8e81-7f6a8f7fffff"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[9150]], dtype=int32)>"]},"execution_count":209,"metadata":{},"output_type":"execute_result"}],"source":["output = tf.reshape(output,shape=(1,1))\n","output"]},{"cell_type":"code","execution_count":210,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689938662249,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"Ubg51L1Y6TX_"},"outputs":[],"source":["results = results.write(0,output)"]},{"cell_type":"code","execution_count":211,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689938662249,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"kCNWuyaX6WPy","outputId":"62cb27ef-6b97-40ec-8d56-949e6f38d6ec"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[9150]], dtype=int32)>"]},"execution_count":211,"metadata":{},"output_type":"execute_result"}],"source":["results.read(0)"]},{"cell_type":"code","execution_count":212,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938662249,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"ad9AyIxU6jXD"},"outputs":[],"source":["results = tf.TensorArray(tf.int32,30)\n","target_seq = tf.constant([[start]],dtype=tf.float32)"]},{"cell_type":"code","execution_count":214,"metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1689938671577,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"_pA5wpiJ5o06"},"outputs":[],"source":["for i in tf.range(30):\n","        output,decoder_hidden, decoder_cell = decoder(target_seq,hidden,cell)\n","        output = tf.cast(tf.argmax(output,axis=-1),dtype=tf.int32)\n","        output = tf.reshape(output,shape=(1,1))\n","        results = results.write(i,output)\n","\n","        # EOS token을 만나면 문장 생성 중지\n","        if output == tokenizer.word_index['<END>']:\n","          break\n","\n","        # 이전 값들 최신화\n","        target_seq = output\n","        hidden = decoder_hidden\n","        cell = decoder_cell"]},{"cell_type":"code","execution_count":215,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1689938674740,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"7CO3MQk9dgm7","outputId":"8d07ca30-9229-4e87-f933-6d8655117ea9"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(30, 1, 1), dtype=int32, numpy=\n","array([[[11278]],\n","\n","       [[ 2015]],\n","\n","       [[ 6711]],\n","\n","       [[12080]],\n","\n","       [[12080]],\n","\n","       [[ 9158]],\n","\n","       [[12277]],\n","\n","       [[ 8070]],\n","\n","       [[10872]],\n","\n","       [[ 9181]],\n","\n","       [[ 9181]],\n","\n","       [[12037]],\n","\n","       [[ 9181]],\n","\n","       [[12037]],\n","\n","       [[ 9181]],\n","\n","       [[ 8140]],\n","\n","       [[ 1957]],\n","\n","       [[12075]],\n","\n","       [[ 2752]],\n","\n","       [[ 1865]],\n","\n","       [[ 2144]],\n","\n","       [[11312]],\n","\n","       [[ 5107]],\n","\n","       [[ 5920]],\n","\n","       [[ 1622]],\n","\n","       [[  988]],\n","\n","       [[ 7794]],\n","\n","       [[ 7794]],\n","\n","       [[  959]],\n","\n","       [[ 5765]]], dtype=int32)>"]},"execution_count":215,"metadata":{},"output_type":"execute_result"}],"source":["results.stack()"]},{"cell_type":"code","execution_count":216,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938675800,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"nrzYoCdx60cY","outputId":"d113adf7-5056-4f4e-c8ef-d4e4a91031c6"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(1, 30), dtype=int32, numpy=\n","array([[11278,  2015,  6711, 12080, 12080,  9158, 12277,  8070, 10872,\n","         9181,  9181, 12037,  9181, 12037,  9181,  8140,  1957, 12075,\n","         2752,  1865,  2144, 11312,  5107,  5920,  1622,   988,  7794,\n","         7794,   959,  5765]], dtype=int32)>"]},"execution_count":216,"metadata":{},"output_type":"execute_result"}],"source":["tf.reshape(results.stack(),shape=(1,30))"]},{"cell_type":"markdown","metadata":{"id":"mhcjGxiPUAaA"},"source":["# 학습"]},{"cell_type":"code","execution_count":217,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938677606,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"CjsvuPA47pTV"},"outputs":[],"source":["EMBEDDING_DIM = 100\n","TIME_STEPS = 30\n","START_TOKEN = tokenizer.word_index['<START>']\n","END_TOKEN = tokenizer.word_index['<END>']\n","\n","UNITS = 128\n","\n","VOCAB_SIZE = len(tokenizer.word_index)+1\n","DATA_LENGTH = len(questions)"]},{"cell_type":"code","execution_count":218,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938678008,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"SSLABYvV7r28"},"outputs":[],"source":["model = Seq2Seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)"]},{"cell_type":"code","execution_count":219,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689938679328,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"eZ5iWhNF7twT"},"outputs":[],"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) # one-hot encoding을 하지 않았기 때문에 sparse_categorical_crossentropy를 사용"]},{"cell_type":"code","execution_count":220,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1689938679328,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"cpIFMcXnYoEd"},"outputs":[],"source":["es = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=3,mode='min',verbose=1)\n","checkpoint_path = 'seq2seq_training_checkpoint.h5'\n","mc = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                             save_weights_only=True,\n","                             save_best_only=True,\n","                             monitor='loss',\n","                             verbose=1\n","                            )"]},{"cell_type":"code","execution_count":221,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":906734,"status":"ok","timestamp":1689939586445,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"BkRKqOcsd1VE","outputId":"a6665d34-1f28-4080-eb40-6cdab561119a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/350\n","185/185 [==============================] - ETA: 0s - loss: 2.5483 - acc: 0.7842\n","Epoch 1: loss improved from inf to 2.54830, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 19s 82ms/step - loss: 2.5483 - acc: 0.7842\n","Epoch 2/350\n","185/185 [==============================] - ETA: 0s - loss: 1.3132 - acc: 0.8161\n","Epoch 2: loss improved from 2.54830 to 1.31325, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 6s 33ms/step - loss: 1.3132 - acc: 0.8161\n","Epoch 3/350\n","185/185 [==============================] - ETA: 0s - loss: 1.2301 - acc: 0.8239\n","Epoch 3: loss improved from 1.31325 to 1.23014, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 5s 26ms/step - loss: 1.2301 - acc: 0.8239\n","Epoch 4/350\n","184/185 [============================>.] - ETA: 0s - loss: 1.1768 - acc: 0.8285\n","Epoch 4: loss improved from 1.23014 to 1.17708, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 5s 26ms/step - loss: 1.1771 - acc: 0.8284\n","Epoch 5/350\n","183/185 [============================>.] - ETA: 0s - loss: 1.1366 - acc: 0.8325\n","Epoch 5: loss improved from 1.17708 to 1.13705, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 24ms/step - loss: 1.1370 - acc: 0.8324\n","Epoch 6/350\n","185/185 [==============================] - ETA: 0s - loss: 1.1069 - acc: 0.8344\n","Epoch 6: loss improved from 1.13705 to 1.10691, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 5s 25ms/step - loss: 1.1069 - acc: 0.8344\n","Epoch 7/350\n","185/185 [==============================] - ETA: 0s - loss: 1.0822 - acc: 0.8368\n","Epoch 7: loss improved from 1.10691 to 1.08220, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 1.0822 - acc: 0.8368\n","Epoch 8/350\n","185/185 [==============================] - ETA: 0s - loss: 1.0577 - acc: 0.8395\n","Epoch 8: loss improved from 1.08220 to 1.05769, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 23ms/step - loss: 1.0577 - acc: 0.8395\n","Epoch 9/350\n","184/185 [============================>.] - ETA: 0s - loss: 1.0321 - acc: 0.8433\n","Epoch 9: loss improved from 1.05769 to 1.03246, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 1.0325 - acc: 0.8433\n","Epoch 10/350\n","185/185 [==============================] - ETA: 0s - loss: 1.0068 - acc: 0.8462\n","Epoch 10: loss improved from 1.03246 to 1.00675, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 23ms/step - loss: 1.0068 - acc: 0.8462\n","Epoch 11/350\n","185/185 [==============================] - ETA: 0s - loss: 0.9807 - acc: 0.8486\n","Epoch 11: loss improved from 1.00675 to 0.98066, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.9807 - acc: 0.8486\n","Epoch 12/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.8507\n","Epoch 12: loss improved from 0.98066 to 0.95574, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.9557 - acc: 0.8507\n","Epoch 13/350\n","185/185 [==============================] - ETA: 0s - loss: 0.9314 - acc: 0.8532\n","Epoch 13: loss improved from 0.95574 to 0.93143, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 24ms/step - loss: 0.9314 - acc: 0.8532\n","Epoch 14/350\n","185/185 [==============================] - ETA: 0s - loss: 0.9085 - acc: 0.8554\n","Epoch 14: loss improved from 0.93143 to 0.90850, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.9085 - acc: 0.8554\n","Epoch 15/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.8575\n","Epoch 15: loss improved from 0.90850 to 0.88737, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.8874 - acc: 0.8575\n","Epoch 16/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8667 - acc: 0.8594\n","Epoch 16: loss improved from 0.88737 to 0.86667, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.8667 - acc: 0.8594\n","Epoch 17/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8469 - acc: 0.8614\n","Epoch 17: loss improved from 0.86667 to 0.84693, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.8469 - acc: 0.8614\n","Epoch 18/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8285 - acc: 0.8631\n","Epoch 18: loss improved from 0.84693 to 0.82847, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.8285 - acc: 0.8631\n","Epoch 19/350\n","185/185 [==============================] - ETA: 0s - loss: 0.8110 - acc: 0.8645\n","Epoch 19: loss improved from 0.82847 to 0.81103, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 5s 25ms/step - loss: 0.8110 - acc: 0.8645\n","Epoch 20/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7941 - acc: 0.8663\n","Epoch 20: loss improved from 0.81103 to 0.79414, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.7941 - acc: 0.8663\n","Epoch 21/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7773 - acc: 0.8677\n","Epoch 21: loss improved from 0.79414 to 0.77733, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 24ms/step - loss: 0.7773 - acc: 0.8677\n","Epoch 22/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7615 - acc: 0.8693\n","Epoch 22: loss improved from 0.77733 to 0.76152, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.7615 - acc: 0.8693\n","Epoch 23/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.8709\n","Epoch 23: loss improved from 0.76152 to 0.74676, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.7468 - acc: 0.8710\n","Epoch 24/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7328 - acc: 0.8726\n","Epoch 24: loss improved from 0.74676 to 0.73277, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.7328 - acc: 0.8726\n","Epoch 25/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7186 - acc: 0.8741\n","Epoch 25: loss improved from 0.73277 to 0.71862, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.7186 - acc: 0.8741\n","Epoch 26/350\n","185/185 [==============================] - ETA: 0s - loss: 0.7052 - acc: 0.8754\n","Epoch 26: loss improved from 0.71862 to 0.70515, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 24ms/step - loss: 0.7052 - acc: 0.8754\n","Epoch 27/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.8772\n","Epoch 27: loss improved from 0.70515 to 0.69241, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6924 - acc: 0.8773\n","Epoch 28/350\n","185/185 [==============================] - ETA: 0s - loss: 0.6800 - acc: 0.8787\n","Epoch 28: loss improved from 0.69241 to 0.67996, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.6800 - acc: 0.8787\n","Epoch 29/350\n","185/185 [==============================] - ETA: 0s - loss: 0.6678 - acc: 0.8802\n","Epoch 29: loss improved from 0.67996 to 0.66783, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.6678 - acc: 0.8802\n","Epoch 30/350\n","185/185 [==============================] - ETA: 0s - loss: 0.6565 - acc: 0.8815\n","Epoch 30: loss improved from 0.66783 to 0.65651, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.6565 - acc: 0.8815\n","Epoch 31/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6447 - acc: 0.8833\n","Epoch 31: loss improved from 0.65651 to 0.64487, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6449 - acc: 0.8833\n","Epoch 32/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.8846\n","Epoch 32: loss improved from 0.64487 to 0.63427, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6343 - acc: 0.8845\n","Epoch 33/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6239 - acc: 0.8864\n","Epoch 33: loss improved from 0.63427 to 0.62404, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6240 - acc: 0.8863\n","Epoch 34/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6148 - acc: 0.8876\n","Epoch 34: loss improved from 0.62404 to 0.61466, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.6147 - acc: 0.8876\n","Epoch 35/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.8892\n","Epoch 35: loss improved from 0.61466 to 0.60513, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.6051 - acc: 0.8891\n","Epoch 36/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.8906\n","Epoch 36: loss improved from 0.60513 to 0.59575, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5957 - acc: 0.8907\n","Epoch 37/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5864 - acc: 0.8917\n","Epoch 37: loss improved from 0.59575 to 0.58642, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5864 - acc: 0.8917\n","Epoch 38/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5790 - acc: 0.8929\n","Epoch 38: loss improved from 0.58642 to 0.57896, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.5790 - acc: 0.8929\n","Epoch 39/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.8945\n","Epoch 39: loss improved from 0.57896 to 0.57053, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5705 - acc: 0.8944\n","Epoch 40/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.5617 - acc: 0.8962\n","Epoch 40: loss improved from 0.57053 to 0.56232, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5623 - acc: 0.8960\n","Epoch 41/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8971\n","Epoch 41: loss improved from 0.56232 to 0.55486, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5549 - acc: 0.8970\n","Epoch 42/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.8981\n","Epoch 42: loss improved from 0.55486 to 0.54729, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5473 - acc: 0.8981\n","Epoch 43/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8993\n","Epoch 43: loss improved from 0.54729 to 0.54015, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5402 - acc: 0.8992\n","Epoch 44/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.9005\n","Epoch 44: loss improved from 0.54015 to 0.53305, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5330 - acc: 0.9005\n","Epoch 45/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.9017\n","Epoch 45: loss improved from 0.53305 to 0.52667, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5267 - acc: 0.9017\n","Epoch 46/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.9028\n","Epoch 46: loss improved from 0.52667 to 0.51972, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5197 - acc: 0.9027\n","Epoch 47/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.9040\n","Epoch 47: loss improved from 0.51972 to 0.51353, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5135 - acc: 0.9040\n","Epoch 48/350\n","185/185 [==============================] - ETA: 0s - loss: 0.5074 - acc: 0.9051\n","Epoch 48: loss improved from 0.51353 to 0.50743, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.5074 - acc: 0.9051\n","Epoch 49/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.9057\n","Epoch 49: loss improved from 0.50743 to 0.50234, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.5023 - acc: 0.9056\n","Epoch 50/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.9070\n","Epoch 50: loss improved from 0.50234 to 0.49593, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4959 - acc: 0.9069\n","Epoch 51/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.9079\n","Epoch 51: loss improved from 0.49593 to 0.49102, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4910 - acc: 0.9078\n","Epoch 52/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4848 - acc: 0.9093\n","Epoch 52: loss improved from 0.49102 to 0.48479, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4848 - acc: 0.9093\n","Epoch 53/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4796 - acc: 0.9098\n","Epoch 53: loss improved from 0.48479 to 0.47956, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 23ms/step - loss: 0.4796 - acc: 0.9098\n","Epoch 54/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4748 - acc: 0.9110\n","Epoch 54: loss improved from 0.47956 to 0.47479, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.4748 - acc: 0.9110\n","Epoch 55/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4691 - acc: 0.9119\n","Epoch 55: loss improved from 0.47479 to 0.46911, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.4691 - acc: 0.9119\n","Epoch 56/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4641 - acc: 0.9130\n","Epoch 56: loss improved from 0.46911 to 0.46415, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4641 - acc: 0.9130\n","Epoch 57/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.9135\n","Epoch 57: loss improved from 0.46415 to 0.45955, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4595 - acc: 0.9135\n","Epoch 58/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.9145\n","Epoch 58: loss improved from 0.45955 to 0.45443, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4544 - acc: 0.9145\n","Epoch 59/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4501 - acc: 0.9151\n","Epoch 59: loss improved from 0.45443 to 0.45006, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4501 - acc: 0.9151\n","Epoch 60/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4458 - acc: 0.9160\n","Epoch 60: loss improved from 0.45006 to 0.44576, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 24ms/step - loss: 0.4458 - acc: 0.9160\n","Epoch 61/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4408 - acc: 0.9170\n","Epoch 61: loss improved from 0.44576 to 0.44079, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4408 - acc: 0.9170\n","Epoch 62/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4363 - acc: 0.9179\n","Epoch 62: loss improved from 0.44079 to 0.43630, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 23ms/step - loss: 0.4363 - acc: 0.9179\n","Epoch 63/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.9184\n","Epoch 63: loss improved from 0.43630 to 0.43164, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4316 - acc: 0.9184\n","Epoch 64/350\n","185/185 [==============================] - ETA: 0s - loss: 0.4275 - acc: 0.9195\n","Epoch 64: loss improved from 0.43164 to 0.42746, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4275 - acc: 0.9195\n","Epoch 65/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.9204\n","Epoch 65: loss improved from 0.42746 to 0.42324, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4232 - acc: 0.9203\n","Epoch 66/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.9208\n","Epoch 66: loss improved from 0.42324 to 0.41878, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4188 - acc: 0.9209\n","Epoch 67/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.9218\n","Epoch 67: loss improved from 0.41878 to 0.41463, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4146 - acc: 0.9217\n","Epoch 68/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.9223\n","Epoch 68: loss improved from 0.41463 to 0.41095, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4110 - acc: 0.9222\n","Epoch 69/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.9232\n","Epoch 69: loss improved from 0.41095 to 0.40710, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.4071 - acc: 0.9231\n","Epoch 70/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.9237\n","Epoch 70: loss improved from 0.40710 to 0.40275, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.4028 - acc: 0.9237\n","Epoch 71/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3990 - acc: 0.9244\n","Epoch 71: loss improved from 0.40275 to 0.39896, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3990 - acc: 0.9244\n","Epoch 72/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.9249\n","Epoch 72: loss improved from 0.39896 to 0.39555, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3955 - acc: 0.9249\n","Epoch 73/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.9255\n","Epoch 73: loss improved from 0.39555 to 0.39122, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3912 - acc: 0.9255\n","Epoch 74/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.9263\n","Epoch 74: loss improved from 0.39122 to 0.38717, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3872 - acc: 0.9262\n","Epoch 75/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.9268\n","Epoch 75: loss improved from 0.38717 to 0.38410, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3841 - acc: 0.9267\n","Epoch 76/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3804 - acc: 0.9276\n","Epoch 76: loss improved from 0.38410 to 0.38035, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3804 - acc: 0.9276\n","Epoch 77/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.9278\n","Epoch 77: loss improved from 0.38035 to 0.37711, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3771 - acc: 0.9278\n","Epoch 78/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.9285\n","Epoch 78: loss improved from 0.37711 to 0.37344, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3734 - acc: 0.9285\n","Epoch 79/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3703 - acc: 0.9290\n","Epoch 79: loss improved from 0.37344 to 0.37029, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3703 - acc: 0.9290\n","Epoch 80/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.9296\n","Epoch 80: loss improved from 0.37029 to 0.36642, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3664 - acc: 0.9295\n","Epoch 81/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.9304\n","Epoch 81: loss improved from 0.36642 to 0.36274, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3627 - acc: 0.9304\n","Epoch 82/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3605 - acc: 0.9305\n","Epoch 82: loss improved from 0.36274 to 0.36047, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3605 - acc: 0.9305\n","Epoch 83/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.9310\n","Epoch 83: loss improved from 0.36047 to 0.35706, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3571 - acc: 0.9310\n","Epoch 84/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.9318\n","Epoch 84: loss improved from 0.35706 to 0.35349, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3535 - acc: 0.9318\n","Epoch 85/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.9322\n","Epoch 85: loss improved from 0.35349 to 0.35059, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3506 - acc: 0.9322\n","Epoch 86/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3473 - acc: 0.9326\n","Epoch 86: loss improved from 0.35059 to 0.34733, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3473 - acc: 0.9326\n","Epoch 87/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3444 - acc: 0.9332\n","Epoch 87: loss improved from 0.34733 to 0.34441, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 24ms/step - loss: 0.3444 - acc: 0.9332\n","Epoch 88/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.9338\n","Epoch 88: loss improved from 0.34441 to 0.34109, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3411 - acc: 0.9338\n","Epoch 89/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.9344\n","Epoch 89: loss improved from 0.34109 to 0.33811, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3381 - acc: 0.9343\n","Epoch 90/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.9345\n","Epoch 90: loss improved from 0.33811 to 0.33540, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3354 - acc: 0.9345\n","Epoch 91/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.9349\n","Epoch 91: loss improved from 0.33540 to 0.33247, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3325 - acc: 0.9349\n","Epoch 92/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.9358\n","Epoch 92: loss improved from 0.33247 to 0.32867, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3287 - acc: 0.9358\n","Epoch 93/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.9360\n","Epoch 93: loss improved from 0.32867 to 0.32656, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3266 - acc: 0.9359\n","Epoch 94/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9362\n","Epoch 94: loss improved from 0.32656 to 0.32306, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3231 - acc: 0.9362\n","Epoch 95/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.9370\n","Epoch 95: loss improved from 0.32306 to 0.31990, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3199 - acc: 0.9370\n","Epoch 96/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9373\n","Epoch 96: loss improved from 0.31990 to 0.31688, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3169 - acc: 0.9373\n","Epoch 97/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9381\n","Epoch 97: loss improved from 0.31688 to 0.31355, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3135 - acc: 0.9381\n","Epoch 98/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9381\n","Epoch 98: loss improved from 0.31355 to 0.31122, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3112 - acc: 0.9381\n","Epoch 99/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3077 - acc: 0.9388\n","Epoch 99: loss improved from 0.31122 to 0.30767, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3077 - acc: 0.9388\n","Epoch 100/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9393\n","Epoch 100: loss improved from 0.30767 to 0.30488, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.3049 - acc: 0.9392\n","Epoch 101/350\n","185/185 [==============================] - ETA: 0s - loss: 0.3015 - acc: 0.9396\n","Epoch 101: loss improved from 0.30488 to 0.30147, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.3015 - acc: 0.9396\n","Epoch 102/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9403\n","Epoch 102: loss improved from 0.30147 to 0.29848, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2985 - acc: 0.9403\n","Epoch 103/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2965 - acc: 0.9403\n","Epoch 103: loss improved from 0.29848 to 0.29648, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2965 - acc: 0.9403\n","Epoch 104/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9409\n","Epoch 104: loss improved from 0.29648 to 0.29363, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2936 - acc: 0.9409\n","Epoch 105/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9415\n","Epoch 105: loss improved from 0.29363 to 0.28981, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2898 - acc: 0.9415\n","Epoch 106/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9417\n","Epoch 106: loss improved from 0.28981 to 0.28741, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2874 - acc: 0.9417\n","Epoch 107/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9422\n","Epoch 107: loss improved from 0.28741 to 0.28434, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2843 - acc: 0.9422\n","Epoch 108/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9427\n","Epoch 108: loss improved from 0.28434 to 0.28207, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2821 - acc: 0.9427\n","Epoch 109/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9430\n","Epoch 109: loss improved from 0.28207 to 0.27911, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2791 - acc: 0.9430\n","Epoch 110/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9437\n","Epoch 110: loss improved from 0.27911 to 0.27537, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.2754 - acc: 0.9437\n","Epoch 111/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9437\n","Epoch 111: loss improved from 0.27537 to 0.27357, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2736 - acc: 0.9437\n","Epoch 112/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9446\n","Epoch 112: loss improved from 0.27357 to 0.27037, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2704 - acc: 0.9445\n","Epoch 113/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9448\n","Epoch 113: loss improved from 0.27037 to 0.26730, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2673 - acc: 0.9448\n","Epoch 114/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9457\n","Epoch 114: loss improved from 0.26730 to 0.26472, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2647 - acc: 0.9457\n","Epoch 115/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9458\n","Epoch 115: loss improved from 0.26472 to 0.26115, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2611 - acc: 0.9458\n","Epoch 116/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9461\n","Epoch 116: loss improved from 0.26115 to 0.25867, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2587 - acc: 0.9461\n","Epoch 117/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9466\n","Epoch 117: loss improved from 0.25867 to 0.25490, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2549 - acc: 0.9466\n","Epoch 118/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9475\n","Epoch 118: loss improved from 0.25490 to 0.25128, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2513 - acc: 0.9475\n","Epoch 119/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9478\n","Epoch 119: loss improved from 0.25128 to 0.24829, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2483 - acc: 0.9478\n","Epoch 120/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2463 - acc: 0.9481\n","Epoch 120: loss improved from 0.24829 to 0.24627, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2463 - acc: 0.9481\n","Epoch 121/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2430 - acc: 0.9486\n","Epoch 121: loss improved from 0.24627 to 0.24305, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2430 - acc: 0.9486\n","Epoch 122/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9492\n","Epoch 122: loss improved from 0.24305 to 0.23969, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2397 - acc: 0.9492\n","Epoch 123/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9495\n","Epoch 123: loss improved from 0.23969 to 0.23741, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2374 - acc: 0.9494\n","Epoch 124/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9500\n","Epoch 124: loss improved from 0.23741 to 0.23420, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2342 - acc: 0.9500\n","Epoch 125/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9504\n","Epoch 125: loss improved from 0.23420 to 0.23184, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2318 - acc: 0.9504\n","Epoch 126/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9506\n","Epoch 126: loss improved from 0.23184 to 0.22911, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2291 - acc: 0.9505\n","Epoch 127/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9511\n","Epoch 127: loss improved from 0.22911 to 0.22590, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2259 - acc: 0.9511\n","Epoch 128/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9515\n","Epoch 128: loss improved from 0.22590 to 0.22347, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2235 - acc: 0.9515\n","Epoch 129/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9523\n","Epoch 129: loss improved from 0.22347 to 0.22054, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2205 - acc: 0.9522\n","Epoch 130/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9521\n","Epoch 130: loss improved from 0.22054 to 0.21987, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2199 - acc: 0.9521\n","Epoch 131/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9527\n","Epoch 131: loss improved from 0.21987 to 0.21616, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2162 - acc: 0.9527\n","Epoch 132/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9534\n","Epoch 132: loss improved from 0.21616 to 0.21179, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2118 - acc: 0.9534\n","Epoch 133/350\n","185/185 [==============================] - ETA: 0s - loss: 0.2090 - acc: 0.9538\n","Epoch 133: loss improved from 0.21179 to 0.20901, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.2090 - acc: 0.9538\n","Epoch 134/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9543\n","Epoch 134: loss improved from 0.20901 to 0.20666, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2067 - acc: 0.9543\n","Epoch 135/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9550\n","Epoch 135: loss improved from 0.20666 to 0.20414, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.2041 - acc: 0.9549\n","Epoch 136/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9558\n","Epoch 136: loss improved from 0.20414 to 0.20172, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.2017 - acc: 0.9557\n","Epoch 137/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9558\n","Epoch 137: loss improved from 0.20172 to 0.19905, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1991 - acc: 0.9558\n","Epoch 138/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9563\n","Epoch 138: loss improved from 0.19905 to 0.19669, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1967 - acc: 0.9563\n","Epoch 139/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9567\n","Epoch 139: loss improved from 0.19669 to 0.19458, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1946 - acc: 0.9567\n","Epoch 140/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9573\n","Epoch 140: loss improved from 0.19458 to 0.19157, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1916 - acc: 0.9573\n","Epoch 141/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9576\n","Epoch 141: loss improved from 0.19157 to 0.18906, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1891 - acc: 0.9576\n","Epoch 142/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9585\n","Epoch 142: loss improved from 0.18906 to 0.18599, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1860 - acc: 0.9585\n","Epoch 143/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9583\n","Epoch 143: loss improved from 0.18599 to 0.18444, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1844 - acc: 0.9583\n","Epoch 144/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9592\n","Epoch 144: loss improved from 0.18444 to 0.18181, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1818 - acc: 0.9592\n","Epoch 145/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9595\n","Epoch 145: loss improved from 0.18181 to 0.17967, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1797 - acc: 0.9594\n","Epoch 146/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9602\n","Epoch 146: loss improved from 0.17967 to 0.17761, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1776 - acc: 0.9602\n","Epoch 147/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.9605\n","Epoch 147: loss improved from 0.17761 to 0.17528, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1753 - acc: 0.9605\n","Epoch 148/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9610\n","Epoch 148: loss improved from 0.17528 to 0.17267, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1727 - acc: 0.9610\n","Epoch 149/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9613\n","Epoch 149: loss improved from 0.17267 to 0.17074, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1707 - acc: 0.9612\n","Epoch 150/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9618\n","Epoch 150: loss improved from 0.17074 to 0.16803, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1680 - acc: 0.9618\n","Epoch 151/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9623\n","Epoch 151: loss improved from 0.16803 to 0.16642, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1664 - acc: 0.9623\n","Epoch 152/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9626\n","Epoch 152: loss improved from 0.16642 to 0.16404, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1640 - acc: 0.9626\n","Epoch 153/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9629\n","Epoch 153: loss improved from 0.16404 to 0.16234, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1623 - acc: 0.9629\n","Epoch 154/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9638\n","Epoch 154: loss improved from 0.16234 to 0.15987, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1599 - acc: 0.9638\n","Epoch 155/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9639\n","Epoch 155: loss improved from 0.15987 to 0.15817, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1582 - acc: 0.9639\n","Epoch 156/350\n","185/185 [==============================] - ETA: 0s - loss: 0.1558 - acc: 0.9644\n","Epoch 156: loss improved from 0.15817 to 0.15583, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1558 - acc: 0.9644\n","Epoch 157/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9645\n","Epoch 157: loss improved from 0.15583 to 0.15427, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1543 - acc: 0.9645\n","Epoch 158/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9650\n","Epoch 158: loss improved from 0.15427 to 0.15263, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1526 - acc: 0.9650\n","Epoch 159/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9659\n","Epoch 159: loss improved from 0.15263 to 0.15005, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1501 - acc: 0.9659\n","Epoch 160/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9662\n","Epoch 160: loss improved from 0.15005 to 0.14816, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1482 - acc: 0.9661\n","Epoch 161/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9664\n","Epoch 161: loss improved from 0.14816 to 0.14607, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1461 - acc: 0.9664\n","Epoch 162/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9671\n","Epoch 162: loss improved from 0.14607 to 0.14434, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1443 - acc: 0.9671\n","Epoch 163/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9673\n","Epoch 163: loss improved from 0.14434 to 0.14244, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1424 - acc: 0.9673\n","Epoch 164/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9676\n","Epoch 164: loss improved from 0.14244 to 0.14048, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1405 - acc: 0.9676\n","Epoch 165/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9677\n","Epoch 165: loss improved from 0.14048 to 0.13919, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1392 - acc: 0.9677\n","Epoch 166/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9683\n","Epoch 166: loss improved from 0.13919 to 0.13619, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1362 - acc: 0.9683\n","Epoch 167/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9687\n","Epoch 167: loss improved from 0.13619 to 0.13546, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1355 - acc: 0.9687\n","Epoch 168/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9692\n","Epoch 168: loss improved from 0.13546 to 0.13361, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1336 - acc: 0.9692\n","Epoch 169/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9696\n","Epoch 169: loss improved from 0.13361 to 0.13216, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1322 - acc: 0.9696\n","Epoch 170/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9698\n","Epoch 170: loss improved from 0.13216 to 0.13067, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1307 - acc: 0.9698\n","Epoch 171/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9704\n","Epoch 171: loss improved from 0.13067 to 0.12834, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1283 - acc: 0.9705\n","Epoch 172/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9699\n","Epoch 172: loss did not improve from 0.12834\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1287 - acc: 0.9699\n","Epoch 173/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9708\n","Epoch 173: loss improved from 0.12834 to 0.12610, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1261 - acc: 0.9708\n","Epoch 174/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9713\n","Epoch 174: loss improved from 0.12610 to 0.12433, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1243 - acc: 0.9713\n","Epoch 175/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9717\n","Epoch 175: loss improved from 0.12433 to 0.12287, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1229 - acc: 0.9716\n","Epoch 176/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9718\n","Epoch 176: loss improved from 0.12287 to 0.12174, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1217 - acc: 0.9718\n","Epoch 177/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9722\n","Epoch 177: loss improved from 0.12174 to 0.11952, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1195 - acc: 0.9721\n","Epoch 178/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9724\n","Epoch 178: loss improved from 0.11952 to 0.11890, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1189 - acc: 0.9725\n","Epoch 179/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9729\n","Epoch 179: loss improved from 0.11890 to 0.11632, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1163 - acc: 0.9729\n","Epoch 180/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9734\n","Epoch 180: loss improved from 0.11632 to 0.11497, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1150 - acc: 0.9733\n","Epoch 181/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9727\n","Epoch 181: loss did not improve from 0.11497\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1162 - acc: 0.9727\n","Epoch 182/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9736\n","Epoch 182: loss improved from 0.11497 to 0.11381, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1138 - acc: 0.9737\n","Epoch 183/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9742\n","Epoch 183: loss improved from 0.11381 to 0.11184, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1118 - acc: 0.9741\n","Epoch 184/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9743\n","Epoch 184: loss improved from 0.11184 to 0.11039, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1104 - acc: 0.9743\n","Epoch 185/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9747\n","Epoch 185: loss improved from 0.11039 to 0.10857, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1086 - acc: 0.9747\n","Epoch 186/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9748\n","Epoch 186: loss improved from 0.10857 to 0.10753, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1075 - acc: 0.9749\n","Epoch 187/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9749\n","Epoch 187: loss improved from 0.10753 to 0.10731, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1073 - acc: 0.9749\n","Epoch 188/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9751\n","Epoch 188: loss improved from 0.10731 to 0.10625, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1063 - acc: 0.9751\n","Epoch 189/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9757\n","Epoch 189: loss improved from 0.10625 to 0.10404, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1040 - acc: 0.9757\n","Epoch 190/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9762\n","Epoch 190: loss improved from 0.10404 to 0.10261, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.1026 - acc: 0.9762\n","Epoch 191/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9766\n","Epoch 191: loss improved from 0.10261 to 0.10120, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.1012 - acc: 0.9766\n","Epoch 192/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9768\n","Epoch 192: loss improved from 0.10120 to 0.09969, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0997 - acc: 0.9768\n","Epoch 193/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9771\n","Epoch 193: loss improved from 0.09969 to 0.09870, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0987 - acc: 0.9770\n","Epoch 194/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9776\n","Epoch 194: loss improved from 0.09870 to 0.09658, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0966 - acc: 0.9776\n","Epoch 195/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9770\n","Epoch 195: loss did not improve from 0.09658\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0975 - acc: 0.9770\n","Epoch 196/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9779\n","Epoch 196: loss improved from 0.09658 to 0.09545, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0955 - acc: 0.9780\n","Epoch 197/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9777\n","Epoch 197: loss did not improve from 0.09545\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0955 - acc: 0.9777\n","Epoch 198/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9778\n","Epoch 198: loss improved from 0.09545 to 0.09439, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0944 - acc: 0.9778\n","Epoch 199/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9785\n","Epoch 199: loss improved from 0.09439 to 0.09199, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0920 - acc: 0.9784\n","Epoch 200/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9786\n","Epoch 200: loss improved from 0.09199 to 0.09124, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0912 - acc: 0.9786\n","Epoch 201/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9790\n","Epoch 201: loss improved from 0.09124 to 0.08989, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0899 - acc: 0.9790\n","Epoch 202/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9793\n","Epoch 202: loss improved from 0.08989 to 0.08887, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0889 - acc: 0.9793\n","Epoch 203/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9791\n","Epoch 203: loss did not improve from 0.08887\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0894 - acc: 0.9790\n","Epoch 204/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9794\n","Epoch 204: loss improved from 0.08887 to 0.08796, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0880 - acc: 0.9794\n","Epoch 205/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9798\n","Epoch 205: loss improved from 0.08796 to 0.08586, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0859 - acc: 0.9798\n","Epoch 206/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9799\n","Epoch 206: loss improved from 0.08586 to 0.08556, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0856 - acc: 0.9799\n","Epoch 207/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9802\n","Epoch 207: loss improved from 0.08556 to 0.08446, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0845 - acc: 0.9802\n","Epoch 208/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9804\n","Epoch 208: loss improved from 0.08446 to 0.08409, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0841 - acc: 0.9804\n","Epoch 209/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9805\n","Epoch 209: loss improved from 0.08409 to 0.08363, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0836 - acc: 0.9804\n","Epoch 210/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9808\n","Epoch 210: loss improved from 0.08363 to 0.08214, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.0821 - acc: 0.9808\n","Epoch 211/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9810\n","Epoch 211: loss improved from 0.08214 to 0.08102, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0810 - acc: 0.9810\n","Epoch 212/350\n","185/185 [==============================] - ETA: 0s - loss: 0.0799 - acc: 0.9813\n","Epoch 212: loss improved from 0.08102 to 0.07989, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0799 - acc: 0.9813\n","Epoch 213/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9810\n","Epoch 213: loss improved from 0.07989 to 0.07943, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0794 - acc: 0.9810\n","Epoch 214/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9814\n","Epoch 214: loss improved from 0.07943 to 0.07900, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0790 - acc: 0.9813\n","Epoch 215/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9817\n","Epoch 215: loss improved from 0.07900 to 0.07762, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 22ms/step - loss: 0.0776 - acc: 0.9817\n","Epoch 216/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9822\n","Epoch 216: loss improved from 0.07762 to 0.07602, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0760 - acc: 0.9822\n","Epoch 217/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9822\n","Epoch 217: loss improved from 0.07602 to 0.07593, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0759 - acc: 0.9822\n","Epoch 218/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9825\n","Epoch 218: loss improved from 0.07593 to 0.07492, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0749 - acc: 0.9825\n","Epoch 219/350\n","185/185 [==============================] - ETA: 0s - loss: 0.0732 - acc: 0.9828\n","Epoch 219: loss improved from 0.07492 to 0.07319, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0732 - acc: 0.9828\n","Epoch 220/350\n","185/185 [==============================] - ETA: 0s - loss: 0.0736 - acc: 0.9825\n","Epoch 220: loss did not improve from 0.07319\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0736 - acc: 0.9825\n","Epoch 221/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9828\n","Epoch 221: loss improved from 0.07319 to 0.07280, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0728 - acc: 0.9828\n","Epoch 222/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9831\n","Epoch 222: loss improved from 0.07280 to 0.07207, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0721 - acc: 0.9830\n","Epoch 223/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9833\n","Epoch 223: loss improved from 0.07207 to 0.07125, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0713 - acc: 0.9833\n","Epoch 224/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9838\n","Epoch 224: loss improved from 0.07125 to 0.06972, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0697 - acc: 0.9837\n","Epoch 225/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9836\n","Epoch 225: loss improved from 0.06972 to 0.06946, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0695 - acc: 0.9836\n","Epoch 226/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9839\n","Epoch 226: loss improved from 0.06946 to 0.06836, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0684 - acc: 0.9839\n","Epoch 227/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9840\n","Epoch 227: loss improved from 0.06836 to 0.06797, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0680 - acc: 0.9840\n","Epoch 228/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9838\n","Epoch 228: loss improved from 0.06797 to 0.06782, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0678 - acc: 0.9838\n","Epoch 229/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9842\n","Epoch 229: loss improved from 0.06782 to 0.06669, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0667 - acc: 0.9842\n","Epoch 230/350\n","183/185 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9846\n","Epoch 230: loss improved from 0.06669 to 0.06512, saving model to seq2seq_training_checkpoint.h5\n","185/185 [==============================] - 4s 21ms/step - loss: 0.0651 - acc: 0.9846\n","Epoch 231/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9846\n","Epoch 231: loss did not improve from 0.06512\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0654 - acc: 0.9846\n","Epoch 232/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9840\n","Epoch 232: loss did not improve from 0.06512\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0680 - acc: 0.9840\n","Epoch 233/350\n","184/185 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9835\n","Epoch 233: loss did not improve from 0.06512\n","185/185 [==============================] - 4s 20ms/step - loss: 0.0690 - acc: 0.9835\n","Epoch 233: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7da7e8199990>"]},"execution_count":221,"metadata":{},"output_type":"execute_result"}],"source":["model.fit([question_padded, answer_in_padded],answer_out_padded,epochs=350,batch_size=64, callbacks=[mc,es])"]},{"cell_type":"markdown","metadata":{"id":"zojpgMg5U74G"},"source":["# 예측"]},{"cell_type":"code","execution_count":222,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1689939586445,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"tK6XvNmceDVw"},"outputs":[],"source":["def convert_index_to_text(indexs, end_token):\n","\n","    sentence = ''\n","\n","    # 모든 문장에 대해서 반복\n","    for index in indexs:\n","        if index == end_token:\n","            # 끝 단어이므로 예측 중비\n","            break;\n","        # 사전에 존재하는 단어의 경우 단어 추가\n","        if index > 0 and tokenizer.index_word[index] is not None:\n","            sentence += tokenizer.index_word[index]\n","        else:\n","        # 사전에 없는 인덱스면 빈 문자열 추가\n","            sentence += ''\n","\n","        # 빈칸 추가\n","        sentence += ' '\n","    return sentence"]},{"cell_type":"code","execution_count":223,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1689939586445,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"jedNcoLfeDmx"},"outputs":[],"source":["def make_prediction(model, question_inputs):\n","    results = model(x=question_inputs, training=False)  # 추론이기 때문에 training=False\n","    # 변환된 인덱스를 문장으로 변환\n","    results = np.asarray(results).reshape(-1)\n","    return results"]},{"cell_type":"code","execution_count":224,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1689939586446,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"EDCpKgaWVFDu"},"outputs":[],"source":["# 자연어 (질문 입력) 대한 전처리 함수\n","def make_question(sentence):\n","    sentence = clean_and_morph(sentence)\n","    question_sequence = tokenizer.texts_to_sequences([sentence])\n","    question_padded = pad_sequences(question_sequence, maxlen=30, truncating='post', padding='post')\n","    return question_padded"]},{"cell_type":"code","execution_count":225,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1689939586447,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"KA5xGWfzVGZg"},"outputs":[],"source":["def run_chatbot(question):\n","    question_inputs = make_question(question)\n","    results = make_prediction(model, question_inputs)\n","    results = convert_index_to_text(results, END_TOKEN)\n","    return results"]},{"cell_type":"code","execution_count":226,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574760,"status":"ok","timestamp":1689940161179,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"7hZQBosYVHlp","outputId":"11f1bc4d-5784-4913-ff4d-16a004b9684b"},"outputs":[{"name":"stdout","output_type":"stream","text":["<< 말을 걸어 보세요!\n","박규찬이 누구야\n",">> 챗봇 응답: 그 는 신 이야 \n","<< 말을 걸어 보세요!\n","박규찬 알아?\n",">> 챗봇 응답: 그 는 신 이야 \n","<< 말을 걸어 보세요!\n","저녁에 뭐 먹을까\n",">> 챗봇 응답: 냉장고 파먹기 해보세요 \n","<< 말을 걸어 보세요!\n","오늘 피곤해\n",">> 챗봇 응답: 속 쓰리겠어요 \n","<< 말을 걸어 보세요!\n","q\n"]}],"source":["while True:\n","    user_input = input('<< 말을 걸어 보세요!\\n')\n","    if user_input == 'q':\n","        break\n","    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689940233936,"user":{"displayName":"이동호","userId":"13068063960271114746"},"user_tz":-540},"id":"yu5Bz5JwSRum"},"outputs":[],"source":["model.load_weights('/content/seq2seq_training_checkpoint.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY0J1dY1SUFY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMwtI7oaCWdCGNYJJtZ4xkA","gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":0}
